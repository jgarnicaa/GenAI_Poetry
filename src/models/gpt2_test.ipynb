{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for GPT2 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 544a11a246a244bb8dd6d0839b874d17\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///../../mlflow.db\") #\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "runs = client.search_runs(experiment_ids=[\"1\"])\n",
    "for run in runs:\n",
    "    print(f\"Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/12 14:14:16 WARNING mlflow.pytorch: Stored model version '2.5.1+cu124' does not match installed PyTorch version '2.1.1+cu121'\n",
      "/home/linux/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "logged_model = f\"../../mlruns/1/{run.info.run_id}/artifacts/models\"\n",
    "# Cargar el modelo\n",
    "model = mlflow.pytorch.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English poem:  Under the moonlight, beneath the moonlight,\n",
      "    A child is lying in bed, in a coffin.\n",
      "    He is the one the baby will never see,\n",
      "the one the newborn girl would see—\n",
      "    And the one only woman will be the mother.\n",
      "He will never forget, his words will never forget—\n",
      "    The boy in bed will never dream—\n",
      "\n",
      "Spanish poem:  Bajo el sol a poco de sus nubes\n",
      "\n",
      "míseras del deseo y de un poco deseo\n",
      "\n",
      "las miradas santiguen.\n",
      "\n",
      "\n",
      "\n",
      "No habrá un tiempo que se ancluen. De habrá un amado\n",
      "\n",
      "un hombre que te enredan. No alza bien que tenga,\n",
      "\n",
      "porque se ha contigo\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"../../model/fine-tuned-gpt2-poetry\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def generate_poem(prompt, max_length=150, temperature=1.2, top_p=0.95):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=temperature, #1.2 #0.7\n",
    "        top_p=top_p, #0.95 #0.9\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"English poem: \",generate_poem(\"Under the moonlight,\"))\n",
    "\n",
    "print(\"Spanish poem: \",generate_poem(\"Bajo el sol\")) #2m55.7s mx_length=100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
