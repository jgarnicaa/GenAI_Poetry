{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning gpt-2 to poem generation\n",
    "\n",
    "In this notebook we're going to fine tune gpt-2 small with our custom poetry dataset (english and spanish)\n",
    "\n",
    "Why use this architecture?\n",
    "- Have multilanguage support\n",
    "- Small LLM model to deploy\n",
    "- Easy to train \n",
    "\n",
    "We're going to use MLFlow to compare different models\n",
    "\n",
    "Model taken from: https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "To use MLFlow: \n",
    "    mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns\n",
    "    mlflow ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Poem'],\n",
       "        num_rows: 25563\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load dataset\n",
    "\n",
    "df_es=load_dataset(\"csv\", data_files={\"train\": \"../../data/ES_corpus/ES_poetry_cleaned.csv\"} )\n",
    "df_en=load_dataset(\"csv\", data_files={\"train\": \"../../data/EN_corpus/EN_poetry_cleaned.csv\"} )\n",
    "\n",
    "dataset=DatasetDict({\"train\": concatenate_datasets([df_es[\"train\"], df_en[\"train\"]])}) ## Merge EN and ES dataset\n",
    "\n",
    "dataset.remove_columns([\"Unnamed: 0\", \"Title\", \"Author\", \"Unnamed: 0.1\", \"Poet\", \"Tags\"]) ## Use only Poem corpus, drop rest of info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") ## DEfine tokenizer and model from gpt2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Poem\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_dataset= dataset.map(tokenize_function, batched=True) ## Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args=TrainingArguments( ## Training arguments for huggingface training\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\" #Deactivate hugging face reports \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/linux/Documents/Proyectos%20personales%20IA/GenAI_Poetry/src/models/mlruns/752725571680088763', creation_time=1739013767145, experiment_id='752725571680088763', last_update_time=1739013767145, lifecycle_stage='active', name='FineTuning-GPT2-Poetry', tags={}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"FineTuning-GPT2-Poetry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training with MLFLow logging\n",
    "with mlflow.start_run():\n",
    "    trainer=Trainer(model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=tokenized_dataset[\"train\"])\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    mlflow.log_param(\"epochs\", training_args.num_train_epochs)\n",
    "    mlflow.log_param(\"batch size\", training_args.per_device_train_batch_size)\n",
    "    mlflow.log_artifact(\"results\")\n",
    "\n",
    "    loss=trainer.evaluate()[\"eval_loss\"]\n",
    "    mlflow.log_metric(\"eval_loss\", loss)\n",
    "\n",
    "    model.save_pretrained('./fine-tuned-gpt2-poetry')\n",
    "\n",
    "    mlflow.pytorch.log_model(model, \"fine-tuned-gpt2-poetry\")\n",
    "\n",
    "print(\"Finished training and logged in MLFlow\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
